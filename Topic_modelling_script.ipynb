{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae905c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from bs4 import BeautifulSoup\n",
    "from gensim import corpora, models\n",
    "from itertools import product, count\n",
    "\n",
    "import time, random, math\n",
    "import requests\n",
    "import nltk\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a5dd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls():\n",
    "    api_key = input('API Key:')\n",
    "    calls = int(input('Number of calls:'))\n",
    "    search_query = input('Search query:')\n",
    "    cse_id = input('Google CSE ID:')\n",
    "    urls = set()\n",
    "    start_ = 1\n",
    "    \n",
    "    service = build('customsearch', 'v1', developerKey = api_key).cse()\n",
    "    \n",
    "    for i in count(1):\n",
    "        if i > calls:\n",
    "            break\n",
    "            \n",
    "        result = service.list(q=search_query, cx=cse_id, start=start_).execute()\n",
    "        for item in result['items']:\n",
    "            urls.add(item['link'])\n",
    "            \n",
    "        start_ += 10\n",
    "        time.sleep(random.uniform(1,2))\n",
    "        \n",
    "    return urls\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9566545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_text(links):\n",
    "    article_tag, article_class = input('Enter article html tag and class separated by a space:').split()\n",
    "    date_tag, date_class = input('Enter date html tag and class separated by a space:').split()\n",
    "    \n",
    "    directory_name = input('Enter folder name. Folder must be in the same directory as py script:')\n",
    "    \n",
    "    for line in links:\n",
    "        url_text = requests.get(line).text\n",
    "        soup = BeautifulSoup(url_text, 'lxml')\n",
    "        \n",
    "        article_text = soup.find(article_tag, class_ = article_class).text\n",
    "        date_published = soup.find(date_tag, class_ = date_class).text\n",
    "        \n",
    "        \n",
    "        with open(f'./{directory_name}/{date_published}.txt', 'w') as f:\n",
    "            f.write(article_text)\n",
    "            \n",
    "        time.sleep(random.uniform(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947d60ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(keyword):\n",
    "     \n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "    lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    \n",
    "    path = input('Enter path to text files:')\n",
    "    filelist = os.listdir(path)\n",
    "    \n",
    "    \n",
    "    texts = []\n",
    "    for file in filelist:\n",
    "    with open(f'{path}/{file}') as f:\n",
    "        content = f.read()\n",
    "        if keyword.lower() in content.lower():\n",
    "            texts.append(content)\n",
    "    \n",
    "    tokens = [tokenizer.tokenize(sentence) for sentence in text]\n",
    "\n",
    "    stopwords_removed = [[word for word in token if word.lower() not in stopwords] for token in tokens]\n",
    "\n",
    "    lemmatized_text = [[lemmatizer.lemmatize(word) for word in words] for words in stopwords_removed]\n",
    "    \n",
    "    return lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878b1172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cv(lemmatized_text):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    \n",
    "    dictionary = corpora.Dictionary(lemmatized_text)\n",
    "    corpus = [dictionary.doc2bow(text) for text in lemmatized_text]\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    \n",
    "    topics = [i for i in range(start, limit, step)]\n",
    "    \n",
    "    for topic in topics:\n",
    "\n",
    "        lda_model = models.LdaModel(corpus_tfidf,\n",
    "                                   id2word=dictionary,\n",
    "                                   num_topics=topic)\n",
    "\n",
    "\n",
    "        coherence_model = models.CoherenceModel(model=lda_model,\n",
    "                                               texts=lemmatized_text,\n",
    "                                               dictionary=dictionary,\n",
    "                                               coherence='c_v')\n",
    "\n",
    "        model_list.append(lda_model)\n",
    "        coherence_values.append(coherence_model.get_coherence())\n",
    "        \n",
    "        plt.plot(topics, coherence_values)\n",
    "        \n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751d3694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topics():\n",
    "    \n",
    "    optimal_topic_no = int(input('Enter your identified optimal topic number:'))\n",
    "    optimal model = model_list[topics.index(optimal_topic_no)]\n",
    "    \n",
    "    topics = [topic for index, topic in optimal_model.print_topics()]\n",
    "    \n",
    "    pattern = re.compile(r'[a-zA-Z]+')\n",
    "    topics_text = pattern.findall(' '.join(topics))\n",
    "    \n",
    "    with open('./topics.txt', 'w') as f:\n",
    "        topics_text = ', '.join(topics_text)\n",
    "        f.write(topics_text)\n",
    "        \n",
    "    print(topics_text)\n",
    "    \n",
    "    return topics_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc93489",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    links = get_urls()\n",
    "    proceed = input(\"Enter 'y' to proceed with webscraping:\")\n",
    "    if proceed.lower() == 'y': \n",
    "        get_article_text(links)\n",
    "    \n",
    "    proceed = input('Proceed with topic modelling? (y/n):')\n",
    "    if proceed.lower() == 'y': \n",
    "        lemmatized_text = preprocessing(keyword) # A keyword you expect all relevant files to contain \n",
    "        model_list, coherence_values = compute_cv(lemmatized_text)\n",
    "        \n",
    "    proceed = input('Proceed and get topics? (y/n):')\n",
    "    if proceed.lower() == 'y': \n",
    "        derived_topics = get_topics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
